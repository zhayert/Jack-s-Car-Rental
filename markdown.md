## def clipped_poisson(lam, k):

主要生成一个clipped_poisson PMF, That means that it will return poisson PMF clipped at max_k with remaining tail probability
placed at max_k.

循环计算泊松分布的每个可能值的概率，并将其存储在一个NumPy数组中。最后，对数组中超过max_k的部分进行处理，将其概率合并到max_k这个位置，确保概率的总和为1。

截断泊松分布的概率质量函数通常用于建模某些离散随机事件的发生概率，特别是在涉及到一定上限的情况下。具体来说，这种分布常常用于描述一个系统中的某个过程，比如汽车租赁问题中每天的新租车数量或归还车辆数量。

在实际应用中，很多时候我们会对可能的事件数量进行截断，因为一些情况下事件数量是有限的，或者我们只对有限数量的事件感兴趣。例如，在汽车租赁问题中，车库中的车辆数量(MAX_CAR = 20)是有限的，而且对于问题的求解，我们可能只关心一定数量范围内的新租车和归还车辆的情况。

因此，通过截断泊松分布，我们可以限制考虑的事件数量范围，而不需要考虑无穷大的可能性。这样做的好处之一是可以减小计算的复杂度，尤其是对于一些强化学习算法，如值迭代或策略迭代，其中需要对状态空间进行离散化处理。

总的来说，截断泊松分布的概率质量函数是为了更好地符合实际问题的场景，并且在求解强化学习问题时能够更高效地进行计算。

在这个上下文中，当新租车或归还车辆的数量超过了 `max_k` 时，概率质量函数被截断。这是因为在实际问题中，有时我们只关心一定数量范围内的事件，超过这个范围的概率可以被合并到这个范围之外的最大值，以简化问题的建模和求解。这有助于减小状态空间的大小，提高算法的计算效率。



## def build_rent_return_pmf(lambda_request, lambda_return, max_cars=3):


这段代码用于构建一个概率质量函数（PMF），表示在给定初始车辆数量的情况下，发生新租车和归还车辆的概率。这段代码是为了解决"jacks_car_rental_policy_iteration"问题而设计的。

具体来说，该函数的输入参数包括`lambda_request`（新租车的泊松分布参数），`lambda_return`（归还车辆的泊松分布参数），以及`max_cars`（车库中最大的车辆数目）。输出是一个三维的NumPy ndarray数组，表示概率质量函数。

数组的每个元素`pmf[init_cars, new_rentals, returns]`表示在初始车辆数量为`init_cars`的情况下，发生`new_rentals`个新租车和`returns`个车辆归还的概率。

具体实现的步骤如下：

1. 遍历所有可能的初始车辆数量（从0到`max_cars`）。
2. 对于每个初始车辆数量，使用`clipped_poisson`函数生成新租车的概率分布，其中`lambda_request`是泊松分布的参数。
3. 遍历可能的新租车数量（从0到初始车辆数量）。
4. 对于每个新租车数量，计算可能的返回车辆数量，然后使用`clipped_poisson`函数生成返回车辆的概率分布，其中`lambda_return`是泊松分布的参数。
5. 遍历可能的返回车辆数量，计算概率质量函数的值，并将其存储在相应的数组元素中。

整个过程的目的是建立一个模型，用于描述在不同初始车辆数量下发生新租车和车辆归还的概率分布，这对于解决“jacks_car_rental_policy_iteration”问题是非常重要的。在问题的上下文中，可能会使用这个概率分布来执行值迭代或策略迭代等强化学习算法。



## class JacksWorld(object):

这段代码定义了一个名为`JacksWorld`的类，表示Jack's Car Rental问题的环境模型。该类的实例可以用于获取状态转移模型和奖励模型，这在强化学习中是值迭代或策略迭代等算法所需的关键组成部分。

具体来说，该类的初始化方法`__init__`接受四个泊松分布的参数（分别是`lambda_return1`、`lambda_return2`、`lambda_request1`和`lambda_request2`）以及最大车辆数量`max_cars`。在初始化过程中，会调用`build_rent_return_pmf`函数来预先构建每个位置（location）的租赁和归还车辆的概率质量函数。

类中的`get_transition_model`方法用于获取状态转移模型和奖励模型。具体操作如下：

1. 首先，根据动作`a`（表示从loc1搬运到loc2的车辆数量），计算新的状态`s`。
2. 然后，对每个位置（*loc1*和*loc2*）和可能的租赁、归还车辆数量进行遍历，计算在给定动作和状态下，新状态`s'`的概率和期望奖励。
3. 最后，将两个位置的结果合并，得到整体的状态转移模型`t_model`和奖励模型`r_model`。

总体来说，这个类的作用是提供一个抽象的环境模型，方便在强化学习算法中使用，特别是用于解决Jack's Car Rental问题。通过提供状态转移模型和奖励模型，可以更容易地实施值迭代、策略迭代等强化学习算法。

其中，状态转移模型Transition Model和奖励模型是描述环境动态的两个关键部分：

1. **状态转移模型**（Transition Model）：
   - 状态转移模型描述了在给定当前状态和执行某个动作后，系统将转移到下一个状态的概率分布。
   - 数学表示为*P*(*s*′∣*s*,*a*)，表示在状态 *s* 下执行动作 *a* 后，转移到状态 *s*‘ 的概率。
   - 在强化学习中，状态转移模型对于值迭代、策略迭代等算法是非常关键的，因为这些算法需要对未来的状态进行预测以进行决策。
2. **奖励模型**（Reward Model）：
   - 奖励模型描述了在给定当前状态、执行某个动作以及转移到下一个状态后，系统会获得的即时奖励的期望。
   - 数学表示为 *R*(*s*,*a*,*s*′)，表示在状态 *s* 下执行动作 *a* 并转移到状态 *s*′ 后，获得的奖励的期望值。
   - 奖励模型是强化学习算法中的目标函数，决定了智能体如何通过最大化累积奖励来学习良好的策略。

在上述代码中，`get_transition_model`方法返回了状态转移模型 `t_model` 和奖励模型 `r_model`。`t_model` 是一个字典，键为下一个状态 `(s_prime1, s_prime2)`，值为转移到该状态的概率。`r_model` 也是一个字典，键为下一个状态 `(s_prime1, s_prime2)`，值为在转移到该状态后获得的奖励的期望值。`t_model`和`r_model`的KEY值是一个元组（tuple），Value是一个数为概率值和奖励期望值。

例如，对于一个特定的 `(s_prime1, s_prime2):prob`，`t_model[(s_prime1, s_prime2)]` 的值表示在执行某个动作后，系统转移到这个状态的概率。这个概率值是由之前计算的租赁和归还车辆的概率质量函数得到的。

具体而言，`r_model` 中的每个键值对 `(s_prime1, s_prime2): reward` 表示在从当前状态执行某个动作后，系统可能转移到状态 `(s_prime1, s_prime2)`，并且在此过程中获得的奖励的期望值为 `reward`。这个期望奖励的计算涉及到对租赁和归还车辆的概率分布的期望值的加权求和。

## \# Initialize environment

这段代码初始化了一个名为 `jacks` 的 `JacksWorld` 类的实例，用于表示 Jack's Car Rental 问题的环境模型。这个环境模型使用了一些参数进行初始化，包括：

- `lambda_return1` 和 `lambda_return2`：表示每天分别在第一个和第二个位置归还车辆的泊松分布参数。
- `lambda_request1` 和 `lambda_request2`：表示每天分别在第一个和第二个位置发生新租车的泊松分布参数。
- `max_cars`：表示车库中的最大车辆数量。

这样的初始化是为了创建一个与具体问题相关的环境实例，该实例可以在强化学习算法中使用，例如值迭代或策略迭代。环境模型中包含了租车和归还车辆的概率质量函数，以及状态转移和奖励模型，这些模型对于强化学习算法的执行非常关键。



## \# Initialize value function and policy

- `pi`: 策略（policy）数组，用于表示在每个状态下采取的动作。在这里，`pi` 是一个二维数组，其中 `pi[s0, s1]` 表示在状态 `(s0, s1)` 下采取的动作数量。初始时所有动作都被初始化为零。
- `V`: V值函数（value function）数组，用于表示每个状态的值。在这里，`V` 是一个二维数组，其中 `V[s0, s1]` 表示在状态 `(s0, s1)` 下的估计值。初始时所有值被初始化为零。
- `states`: 包含所有可能状态的列表。在这里，状态是一个元组 `(s0, s1)`，其中 `s0` 表示第一个位置的车辆数量，`s1` 表示第二个位置的车辆数量。列表 `states` 包含了所有可能的状态组合。
- `gamma`: 折扣因子，表示对未来奖励的重要性。在强化学习中，通过调整 `gamma` 的值可以控制对未来奖励的重视程度。

## ## main loop in Policy Iteration

The main loop function encompasses the entire process of policy iteration, including policy evaluation and policy improvement. The following are the main steps of the code. 

1. **初始化策略 `pi` 和值函数 `V`**：在前面的代码中已经完成了，初始化了策略和值函数。
2. **开始策略迭代的主循环**：主循环执行策略评估和策略改进，直到达到稳定的最优策略。
3. **策略评估**：在每次迭代中，通过使用当前策略 `pi` 来更新值函数 `V`，直到值函数收敛。在这里，使用了最大值迭代（Value Iteration）的思想，即计算每个状态的新值时同时使用了最新估计的值。
4. **策略改进**：在每次迭代中，通过比较不同动作的收益，更新策略 `pi`。对每个状态，选择使得期望累积奖励最大的动作。
5. **检查策略是否稳定**：在每次迭代后检查新的策略是否与上一次的策略相同，如果相同则说明策略已经收敛到最优策略，可以提前结束算法。

在循环的每一步，代码都会输出当前策略 `pi`，并且在策略评估阶段打印值函数更新的最大差异。循环在策略稳定或达到最大迭代次数时结束。

### 1. 策略评估（Policy Evaluation）

策略评估的目标是找到给定策略下的值函数估计。在这里，我们使用迭代法来更新值函数，直到值函数收敛。具体步骤如下：

- **初始化**：初始化值函数 `V` 和一个收敛阈值 `theta`。在代码中，使用了 `V` 和 `theta` 变量。

- **循环迭代**：重复以下步骤，直到值函数收敛（即变化小于 `theta`）或达到最大迭代次数：

  - 对每个状态 `s`进行迭代：

     - 通过状态转移模型和奖励模型，根据当前策略 `pi` 更新值函数 `V[s]`。
- 计算当前状态的值函数变化 `delta`，并检查是否小于阈值 `theta`。
    - 如果 `delta < theta`，则退出循环。

### 2. 策略改进（Policy Improvement）

策略改进的目标是根据新的值函数来改进策略。具体步骤如下：

- **对每个状态进行迭代**：

  - 对于每个状态 `s`：
    - 保存旧策略 `old_action = pi[s]`。
    - 对于可能的动作 `a`，计算在当前状态执行动作 `a` 后的值函数，选择使得期望累积奖励最大的动作作为新的动作 `pi[s]`。
    - 如果新的动作 `pi[s]` 与旧动作 `old_action[s]` 不同，说明策略还在变化，将 `stable` 标记设为 `False`。

- **检查策略是否稳定**：
- 如果在整个迭代中策略没有发生变化，说明策略已经稳定，将`stable`标记为`True`，可以提前退出主循环。





##  # Policy Iteration









**Policy Iteration通常是policy evaluation+policy improvement交替执行直到收敛。 Value iteration通常是寻找Optimal value function+一次policy extraction，它们不用交替执行，因为值函数最优，策略通常也是最优 联系：策略迭代和值迭代**都属于动态规划算法。 值迭代算法是策略评估过程只进行一次迭代的策略迭代算法。

